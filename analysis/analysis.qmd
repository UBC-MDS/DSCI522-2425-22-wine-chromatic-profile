---
title: Wine Chromatic Profile Prediction Project
author: "Adrian Leung, Daria Khon, Farhan Bin Faisal & Zhiwei Zhang"
format: 
  html:
    toc: true
    toc-depth: 2
    embed-resources: true
    fig-numbering: true
    tbl-numbering: true
  pdf:
    toc: true
    toc-depth: 2
    fig-numbering: true
    tbl-numbering: true
editor: source
bibliography: references.bib
jupyter:
  kernelspec:
    name: wine
    language: python
    display_name: Python (wine)
execute:
  echo: false
  warning: false
---

```{python}
import pickle
import pandas as pd 
import numpy as np 
from tabulate import tabulate
from IPython.display import Markdown, display
```


```{python}
# loading data 
wine_train = pd.read_csv("../data/wine_train.csv")
wine_test = pd.read_csv("../data/wine_test.csv")
```

```{python}
info_df = pd.read_csv("../results/tables/feature_datatypes.csv")

summary_df = pd.read_csv("../results/tables/summary_statistics.csv")
```

```{python}
with open("../results/models/wine_pipeline.pickle", "rb") as file:
    pipe = pickle.load(file)

# Running cross validation on default model
cross_validation_results = pd.read_csv("../results/tables/cross_validation.csv").rename(
    columns={"Unnamed: 0": "Scoring Metric"}
)

# Hyperparameter optimization
with open("../results/models/wine_random_search.pickle", "rb") as file:
    random_search = pickle.load(file)

random_search_results = pd.read_csv("../results/tables/random_search.csv")

best_params = random_search.best_params_["logisticregression__C"]
```

```{python}
X_test = wine_test.drop(columns = ["color"])
y_test = wine_test["color"]
best_model_accuracy = random_search.score(X_test, y_test)
```

___   
## SUMMARY   
___   

In this project, we developed a machine learning model to predict the color of wine (red or white) using its physiochemical properties such as acidity, pH, sugar content, and alcohol level. A logistic regression model with balanced class weights was implemented and optimized through hyperparameter tuning. The final model performed exceptionally well, achieving an accuracy of `{python} round(best_model_accuracy, 2)` on unseen test data. The precision-recall analysis indicated high precision and recall scores, consistently above 0.98, further corroborated by the confusion matrix, which showed minimal misclassifications.

While the model demonstrated strong predictive accuracy, the near-perfect results raise potential concerns about overfitting, suggesting further evaluation on truly unseen data is necessary. This work emphasizes the potential for data-driven tools to optimize wine classification processes, offering a scalable and efficient approach for the wine industry.

___
## INTRODUCTION  
___   

Wine classification plays a crucial role in both production and quality assessment, yet traditional methods often rely on subjective evaluations by experts. Therefore, this project seeks to answer **whether we accurately predict the color of wine using its physiochemical properties**

Developing a machine learning model for wine classification has several advantages. For winemakers, it could provide a scalable method for analyzing large datasets, identifying trends, and optimizing production processes. For consumers and retailers, it could serve as a tool to verify wine characteristics without requiring advanced laboratory equipment. Through this project, we aim to contribute to the industry's adoption of data-driven approaches, enabling efficient, reproducible, and cost-effective methods for wine analysis.

___  
## METHODS
___

### Data

The dataset for this project is sourced from the UCI Machine Learning Repository [@dua2017uci] and focuses on wines from the Vinho Verde region in Portugal. It includes 11 physiochemical attributes, such as fixed acidity, volatile acidity, pH, and alcohol content, collected from 1,599 red wine samples and 4,898 white wine samples.

### Analysis
The logistic regression algorithm was used to build a classification model to predict whether a wine sample is red or white (as defined by the `color` column in the dataset). All 11 physiochemical features in the dataset, including fixed acidity, volatile acidity, pH, and alcohol content, were utilized for model training. The dataset was split into 70% for the training set and 30% for the test set.  

Preprocessing steps included removing duplicate entries, ordinal encoding for the `quality` feature, and standardizing all numeric features to ensure uniform scaling. A randomized search with 10-fold cross-validation was conducted, using F1 as the scoring metric, to fine-tune the regularization parameter (`C`). This process helped minimize classification bias and maximize accuracy while identifying the optimal model. Balanced class weights were employed to address potential class imbalances in the dataset.  

The Python programming language [@python3reference] and the following libraries were utilized for the analysis: NumPy [@harris2020numpy] for numerical computations, Pandas [@mckinney2010pandas] for data manipulation, Altair [@vanderplas2018altair] for visualization, and scikit-learn [@pedregosa2011scikit] for model development and evaluation. The complete analysis code is available on GitHub: <https://github.com/UBC-MDS/DSCI522-2425-22-wine-quality.git>.

___

### Exploratory Data Analysis

The dataset was initially loaded into a single dataframe, and several data validation checks were performed to check the quality of the dataset. An exploratory data analysis was conducted afterwards to understand the distribution of features. This included checking for class imbalance in the target variable, examining collinearity between input features, and identifying the types of features present in the dataset. This analysis informed decisions regarding feature encoding in subsequent steps.


To examine the data, we first looked at the data types of columns we will be working with: all features are numeric without any null values. Next, in order to select features for modelling, we examined each feature's distribution in respect to color and feaature correlations.



```{python}
# #| label: tbl-feat-datatypes
# #| tbl-cap: Feature Datatypes
# Markdown(info_df.to_markdown())
```

```{python}
# #| label: tbl-describe
# #| tbl-cap: Summary Statistics
# Markdown(summary_df.to_markdown())
```

![Distribution of Features per Target Class](../results/figures/feature_densities_by_class.png){#fig-feat-dist}

![Correlation between Wine Color Prediction Features](../results/figures/feature_correlation.png){#fig-feat-cor}

The class distributions for all predictors across all measurements show some overlap but display noticeable differences in their central tendencies and variability. The quality feature appears to have a tri-modal distribution and does not exhibit distinct differences based on the target class. However, it could still offer valuable predictive insights through interactions with other features. Therefore, we decided to build our predictive model with all the available features. 
<br>
#### We will next check, feature-feature correlations and feature-target correlations to check for any anomalous correlations and prevent multicolinearity and overfitting in our model.
If any anomalous correlations are found, the script will throw and error and we will examine the features in question and determine whether the best plan of action would be dropping the feature or not.

```{python}
# imports for correlation anomality validations
```

```{python}
#| label: tbl-cross-val
#| tbl-cap: Cross Validation Results
Markdown(cross_validation_results.to_markdown())
```

```{python}
#| label: tbl-random-search
#| tbl-cap: Random Search Results
Markdown(random_search_results.to_markdown())
```

#### To validate the expected distribution of the target variable, we will use the Prediction Drift check to compare the predicted distributions of the target variable between the training and testing datasets. This helps identify whether the model's predictions are consistent across these datasets or if there is a significant drift.

```{python}
# target_drift_score = pd.read_csv("../results/tables/drift_score.csv")

```

The validation check is successful, and we can see that the modelâ€™s predictions on the training and testing datasets are consistent (as per low Prediction Dirift Socre) and align with the expected target distribution. The actual and predicted target class distributions match the expected proportions, ensuring that the dataset retains its intended balance of classes.

----
The analysis was performed using Python [@python3reference] and the following libraries: requests [@reitz2011requests] for data retrieval, zipfile [@vanrossum2009python] for handling compressed files, numpy [@harris2020numpy] for numerical operations, pandas [@mckinney2010pandas] for data manipulation, altair [@vanderplas2018altair] for data visualization, and scikit-learn [@pedregosa2011scikit] for model implementation and evaluation.

___
### RESULTS
___

Accuracy score on the test data was `{python} best_model_accuracy`

![Confusion Matric](../results/figures/confusion_matrix.png){#fig-confusion}

![Precision Recall Curve](../results/figures/pr_curve.png){#fig-pr}

___
### DISCUSSION
___
From the results above, we see that the logistic regression with balanced class weight performs very well in the validation set as it scores more than 0.98 for all metrics, showing both a high performance in accuracy and much less classification bias due to very high recall and precision scores. Since the validation accuracy score is very high (>0.994), we decide to do a further randomized search cross-validation using the F1 score to find an optimized hyperparameter C that can minimize our classification bias.  

Using the optimized model on the test data, we see that the model has a high accuracy score as well (0.985). Moreover, from the confusion matrix, we see that there are very few false positives (false red) and false negatives (false white) compared to the true ones. This implies that the recall and precision scores are near 1 on the test data. This is further supported by the precision-recall curve as the average precision is 0.99 and the precision score maintains constantly very close to 1 when adjusting the threshold in the model.  

With the results being near perfect with the predictions on the test data, this is out of our expectations as we would expect there will be a more flawed prediction that will not perform as great in both recall and precision in general (especially not when both recall and precision are higher than 0.98). 

The near perfect scores in the test data suggest that our model will probably do a great job in predicting the wine type given any new data. However, we also have to bear in mind that the possibility that the model might not perform as well on actual unseen data. These very high scores are quite alarming that we might have to worry that it somehow overfits on both our train and test data and cannot generalize over new unseen data.  

___
## REFERENCES