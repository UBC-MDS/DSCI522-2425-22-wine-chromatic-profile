---
title: Wine Chromatic Profile Prediction Project
author: "Adrian Leung, Daria Khon, Farhan Bin Faisal & Zhiwei Zhang"
format: 
  html:
    toc: true
    embed-resources: true
  pdf:
    toc: true
    fig-pos: "H"
editor: source
bibliography: references.bib
jupyter:
  kernelspec:
    name: wine
    language: python
    display_name: Python (wine)
echo: false
---

### Group 22

___
### INTRODUCTION
___

This project seeks to answer the following question:   
**"Can we predict the color of wine using its physiochemical properties, such as acidity, sugar content, and alcohol level?"** 

To answer this question, we are using a data set [@dua2017uci] from the Vinho Verde region in Portugal . These datasets, publicly available through the UCI Machine Learning Repository, include 11 physiochemical attributes for each wine sample—such as fixed acidity, volatile acidity, pH, and alcohol content—along with a sensory-based quality score ranging from 0 to 10. The red wine dataset contains 1,599 samples, while the white wine dataset includes 4,898 samples, enabling a comprehensive analysis across different types of wine.

The ability to accurately classify wine color using machine learning could offer several advantages. For instance, winemakers and researchers could efficiently analyze large datasets, identify trends, and optimize production processes. Furthermore, consumers or wine retailers might use such tools to assess or verify wine characteristics without requiring advanced laboratory equipment. By developing a robust classification model, we aim to contribute to the wine industry’s growing adoption of data-driven methods, enhancing efficiency and accuracy in identifying and categorizing wines. Ultimately, this approach could lead to scalable, reproducible, and cost-effective methods for wine analysis.

___  
### METHODS
___

The code for this analysis and report is available on GitHub: <https://github.com/UBC-MDS/DSCI522-2425-22-wine-quality.git>.

The dataset was initially loaded into a single dataframe, and several data validation checks were performed to check the quality of the dataset. An exploratory data analysis was conducted afterwards to understand the distribution of features. This included checking for class imbalance in the target variable, examining collinearity between input features, and identifying the types of features present in the dataset. This analysis informed decisions regarding feature encoding in subsequent steps.

```{python}
import os
import pandas as pd 
import numpy as np 
import pandera as pa
import altair as alt
import altair_ally as aly
from ucimlrepo import fetch_ucirepo 
from sklearn.model_selection import train_test_split
```

```{python}
# loading data 
wine_quality = fetch_ucirepo(id=186) 
wine = pd.DataFrame(wine_quality.data.original) 
wine.to_csv("../data/wine.csv", index=False)
wine = pd.read_csv("../data/wine.csv")

wine = wine.drop_duplicates()
# split data into train and test for EDA 
train_df, test_df = train_test_split(wine, test_size=0.3, shuffle=True, random_state=123)

wine_train = pd.read_csv("../data/wine_train.csv")
wine_test = pd.read_csv("../data/wine_test.csv")
```

### Exploratory Data Analysis

To examine the data, we first looked at the data types of columns we will be working with: all features are numeric without any null values. Next, in order to select features for modelling, we examined each feature's distribution in respect to color and feaature correlations.

```{python}
print("Feature Datatypes\n")
print(wine_train.info())
```

```{python}
print("Summary Statistics\n")
wine_train.describe()
```

```{python}
aly.alt.data_transformers.enable('vegafusion')

print("Figure 1: Distribution of Features per Target Class")
aly.dist(train_df, color = "color")
```

```{python}
print("Figure 2: Correlation between Wine Color PRediction Features\n")
aly.corr(train_df)
```

The class distributions for all predictors across all measurements show some overlap but display noticeable differences in their central tendencies and variability. The quality feature appears to have a tri-modal distribution and does not exhibit distinct differences based on the target class. However, it could still offer valuable predictive insights through interactions with other features. Therefore, we decided to build our predictive model with all the available features. <br>
#### We will next check, feature-feature correlations and feature-target correlations to check for any anomalous correlations and prevent multicolinearity and overfitting in our model.
If any anomalous correlations are found, the script will throw and error and we will examine the features in question and determine whether the best plan of action would be dropping the feature or not.

```{python}
# imports for correlation anomality validations
from deepchecks.tabular import Dataset
from deepchecks.tabular.checks import FeatureLabelCorrelation, FeatureFeatureCorrelation, PredictionDrift

wine_train_ds = Dataset(train_df, label="color", cat_features=[])
wine_test_ds = Dataset(test_df, label="color", cat_features=[])

check_feat_lab_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.8)
check_feat_lab_corr_result = check_feat_lab_corr.run(dataset=wine_train_ds)

if not check_feat_lab_corr_result.passed_conditions():
    raise ValueError("Feature-Label correlation exceeds the maximum acceptable threshold.")

check_feat_feat_corr = FeatureFeatureCorrelation().add_condition_max_number_of_pairs_above_threshold(threshold = 0.8, n_pairs = 0)
check_feat_feat_corr_result = check_feat_feat_corr.run(dataset=wine_train_ds)

if not check_feat_feat_corr_result.passed_conditions():
    raise ValueError("Feature-feature correlation exceeds the maximum acceptable threshold.")
```


```{python}
# imports for modelling
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import (
    ConfusionMatrixDisplay, PrecisionRecallDisplay, 
    make_scorer, recall_score, precision_score, f1_score
)
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.model_selection import (
    cross_validate,
    RandomizedSearchCV,
)
from scipy.stats import loguniform
```

```{python}
# train_df, test_df = train_test_split(wine, test_size=0.3, shuffle=True, random_state=123)
train_df.head()
```

```{python}
X_train = train_df.drop(columns=["color"])
X_test = test_df.drop(columns=["color"])
y_train = train_df["color"]
y_test = test_df["color"]
```

```{python}
# Transformations for different scales of the features
categorical_features = ["color"]
ordinal_features = ["quality"]
numerical_features = [col for col in wine.columns if col != "color" and col != "quality"]

preprocessor = make_column_transformer(
    (OrdinalEncoder(dtype=int), ordinal_features),
    (StandardScaler(), numerical_features),
    remainder='passthrough' 
)
```

```{python}
pipe = make_pipeline(
    preprocessor, 
    LogisticRegression(random_state=123, max_iter=1000, class_weight="balanced"),
)
pipe.fit(X_train, y_train)
```

```{python}
scoring = {
    "accuracy": 'accuracy',
    'precision': make_scorer(precision_score, pos_label = 'red'),
    'recall': make_scorer(recall_score, pos_label = 'red'),
    'f1': make_scorer(f1_score, pos_label = 'red')
}

# Running cross validation on default model
pd.DataFrame(
    cross_validate(pipe, X_train, y_train, return_train_score = True, cv = 5, scoring = scoring)
).agg(['mean', 'std']).round(3).T
```

```{python}
# Hyperparameter optimization
param_grid = {
    "logisticregression__C": loguniform(1e-1, 10)
}

random_search = RandomizedSearchCV(
    pipe,
    param_grid,
    n_iter = 10,
    verbose = 1,
    n_jobs = -1,
    random_state = 123,
    return_train_score = True, 
    scoring = make_scorer(f1_score, pos_label = 'red')
)
random_search.fit(X_train, y_train)
```

```{python}
pd.DataFrame(random_search.cv_results_)[
    [
        "mean_train_score",
        "mean_test_score",
        "param_logisticregression__C",
        "mean_fit_time",
        "rank_test_score"
    ]
].set_index("rank_test_score").sort_index().T
```

```{python}
pd.DataFrame([random_search.best_params_])
```

#### To validate the expected distribution of the target variable, we will use the Prediction Drift check to compare the predicted distributions of the target variable between the training and testing datasets. This helps identify whether the model's predictions are consistent across these datasets or if there is a significant drift.

```{python}
target_drift_check = PredictionDrift()

expected_distribution = {"red": 0.25, "white": 0.75} 
actual_distribution = train_df['color'].value_counts(normalize=True).to_dict()

for cat, prob in expected_distribution.items():
    if abs(actual_distribution.get(cat) - prob) > 0.1:
        print(f"Class '{cat}' deviates significantly from the expected distribution {prob}.")

target_dist_result = target_drift_check.run(
    wine_train_ds, 
    wine_test_ds, 
    model= random_search.best_estimator_
)
pd.DataFrame([target_dist_result.reduce_output()])
```

The validation check is successful, and we can see that the model’s predictions on the training and testing datasets are consistent (as per low Prediction Dirift Socre) and align with the expected target distribution. The actual and predicted target class distributions match the expected proportions, ensuring that the dataset retains its intended balance of classes.

----
The analysis was performed using Python [@python3reference] and the following libraries: requests [@reitz2011requests] for data retrieval, zipfile [@vanrossum2009python] for handling compressed files, numpy [@harris2020numpy] for numerical operations, pandas [@mckinney2010pandas] for data manipulation, altair [@vanderplas2018altair] for data visualization, and scikit-learn [@pedregosa2011scikit] for model implementation and evaluation.

___
### RESULTS
___

```{python}
print("Accuracy score on test data", random_search.score(X_test, y_test))

# print confusion matrix
ConfusionMatrixDisplay.from_estimator(
    random_search,
    X_test,
    y_test,
    values_format="d"
);
```

```{python}
PrecisionRecallDisplay.from_estimator(
    random_search,
    X_test,
    y_test,
    pos_label="red",
    name='wine_quality', 
)
```

___
### DISCUSSION
___
From the results above, we see that the logistic regression with balanced class weight performs very well in the validation set as it scores more than 0.98 for all metrics, showing both a high performance in accuracy and much less classification bias due to very high recall and precision scores. Since the validation accuracy score is very high (>0.994), we decide to do a further randomized search cross-validation using the F1 score to find an optimized hyperparameter C that can minimize our classification bias.  

Using the optimized model on the test data, we see that the model has a high accuracy score as well (0.985). Moreover, from the confusion matrix, we see that there are very few false positives (false red) and false negatives (false white) compared to the true ones. This implies that the recall and precision scores are near 1 on the test data. This is further supported by the precision-recall curve as the average precision is 0.99 and the precision score maintains constantly very close to 1 when adjusting the threshold in the model.  

With the results being near perfect with the predictions on the test data, this is out of our expectations as we would expect there will be a more flawed prediction that will not perform as great in both recall and precision in general (especially not when both recall and precision are higher than 0.98). 

The near perfect scores in the test data suggest that our model will probably do a great job in predicting the wine type given any new data. However, we also have to bear in mind that the possibility that the model might not perform as well on actual unseen data. These very high scores are quite alarming that we might have to worry that it somehow overfits on both our train and test data and cannot generalize over new unseen data.  

___
## REFERENCES